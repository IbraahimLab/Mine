{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d28781d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matplotlib version: 3.10.8\n",
      "torch version: 2.5.1\n",
      "tiktoken version: 0.12.0\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "print(\"matplotlib version:\", version(\"matplotlib\"))\n",
    "print(\"torch version:\", version(\"torch\"))\n",
    "print(\"tiktoken version:\", version(\"tiktoken\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978c011c",
   "metadata": {},
   "source": [
    "### Next chapter i am going to implement the gpt from scratch so i just complete the building blocks of the transformer architecture Only "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,    # Vocabulary size\n",
    "    \"context_length\": 1024, # Context length\n",
    "    \"emb_dim\": 768,         # Embedding dimension\n",
    "    \"n_heads\": 12,          # Number of attention heads\n",
    "    \"n_layers\": 12,         # Number of layers\n",
    "    \"drop_rate\": 0.1,       # Dropout rate\n",
    "    \"qkv_bias\": False       # Query-Key-Value bias\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0001f28",
   "metadata": {},
   "source": [
    "Configuration details for the 124 million parameter GPT-2 model include:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0af3fb",
   "metadata": {},
   "source": [
    "    We use short variable names to keep the code concise. The configuration parameters are defined as follows:\n",
    "\n",
    "* **`vocab_size`**: 50,257 (BPE tokenizer, Chapter 2).\n",
    "* **`context_length`**: Maximum input token count (positional embeddings, Chapter 2).\n",
    "* **`emb_dim`**: Embedding size for token inputs (768-dimensional vector).\n",
    "* **`n_heads`**: Number of attention heads (Chapter 3).\n",
    "* **`n_layers`**: Number of transformer blocks.\n",
    "* **`drop_rate`**: Dropout intensity (e.g., 0.1 for 10%) to mitigate overfitting (Chapter 3).\n",
    "* **`qkv_bias`**: Determines if Linear layers include a bias vector for Q, K, and V. While modern LLMs often disable this, we'll revisit it in Chapter 5 for GPT-2 weight compatibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432ec9a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "436aa4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class DummyGPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        \n",
    "        # Use a placeholder for TransformerBlock\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[DummyTransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "        \n",
    "        # Use a placeholder for LayerNorm\n",
    "        self.final_norm = DummyLayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(\n",
    "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
    "        )\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        x = tok_embeds + pos_embeds\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "class DummyTransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        # A simple placeholder\n",
    "\n",
    "    def forward(self, x):\n",
    "        # This block does nothing and just returns its input.\n",
    "        return x\n",
    "\n",
    "\n",
    "class DummyLayerNorm(nn.Module):\n",
    "    def __init__(self, normalized_shape, eps=1e-5):\n",
    "        super().__init__()\n",
    "        # The parameters here are just to mimic the LayerNorm interface.\n",
    "\n",
    "    def forward(self, x):\n",
    "        # This layer does nothing and just returns its input.\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0172b10c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "batch = []\n",
    "\n",
    "txt1 = \"Every effort moves you\"\n",
    "txt2 = \"Every day holds a\"\n",
    "\n",
    "batch.append(torch.tensor(tokenizer.encode(txt1)))\n",
    "batch.append(torch.tensor(tokenizer.encode(txt2)))\n",
    "batch = torch.stack(batch, dim=0)\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([2, 4, 50257])\n",
      "tensor([[[-0.9289,  0.2748, -0.7557,  ..., -1.6070,  0.2702, -0.5888],\n",
      "         [-0.4476,  0.1726,  0.5354,  ..., -0.3932,  1.5285,  0.8557],\n",
      "         [ 0.5680,  1.6053, -0.2155,  ...,  1.1624,  0.1380,  0.7425],\n",
      "         [ 0.0447,  2.4787, -0.8843,  ...,  1.3219, -0.0864, -0.5856]],\n",
      "\n",
      "        [[-1.5474, -0.0542, -1.0571,  ..., -1.8061, -0.4494, -0.6747],\n",
      "         [-0.8422,  0.8243, -0.1098,  ..., -0.1434,  0.2079,  1.2046],\n",
      "         [ 0.1355,  1.1858, -0.1453,  ...,  0.0869, -0.1590,  0.1552],\n",
      "         [ 0.1666, -0.8138,  0.2307,  ...,  2.5035, -0.3055, -0.3083]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model = DummyGPTModel(GPT_CONFIG_124M)\n",
    "\n",
    "logits = model(batch)\n",
    "print(\"Output shape:\", logits.shape)\n",
    "print(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d972482",
   "metadata": {},
   "source": [
    "### Layer normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3])\n",
      "--------------\n",
      "batch=1, features=3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.tensor([[1.0, 2.0, 3.0]])\n",
    "\n",
    "print(x.shape)\n",
    "print(\"--------------\")\n",
    "print(\"batch=1, features=3\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1908d57",
   "metadata": {},
   "source": [
    "### Step 1: Compute the mean (per row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.]])\n"
     ]
    }
   ],
   "source": [
    "mean = x.mean(dim=-1, keepdim=True)\n",
    "print(mean)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e6ff49",
   "metadata": {},
   "source": [
    "###    μ= (1+2+3) / 3 = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d87c95",
   "metadata": {},
   "source": [
    "### Step 2: Compute the variance (per row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b37a446e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6667]])\n"
     ]
    }
   ],
   "source": [
    "var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "print(var)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263d903b",
   "metadata": {},
   "source": [
    "Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2c0b9923",
   "metadata": {
    "vscode": {
     "languageId": "latex"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \n",
       "\\sigma^2\n",
       "= \\frac{(1-2)^2 + (2-2)^2 + (3-2)^2}{3}\n",
       "= \\frac{2}{3}\n",
       "\\approx 0.6667\n",
       "$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Math\n",
    "\n",
    "display(Math(r\"\"\"\n",
    "\\sigma^2\n",
    "= \\frac{(1-2)^2 + (2-2)^2 + (3-2)^2}{3}\n",
    "= \\frac{2}{3}\n",
    "\\approx 0.6667\n",
    "\"\"\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4b20bf",
   "metadata": {
    "vscode": {
     "languageId": "latex"
    }
   },
   "source": [
    "## Step 3: Normalise (subtract mean, divide by std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7899831a",
   "metadata": {
    "vscode": {
     "languageId": "latex"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.2247,  0.0000,  1.2247]])\n"
     ]
    }
   ],
   "source": [
    "eps = 1e-5\n",
    "x_norm = (x - mean) / torch.sqrt(var + eps)\n",
    "print(x_norm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a80dd6",
   "metadata": {
    "vscode": {
     "languageId": "latex"
    }
   },
   "source": [
    "### Step 4: Check mean and variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ab8f6aee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.])\n",
      "tensor([1.0000])\n"
     ]
    }
   ],
   "source": [
    "print(x_norm.mean(dim=-1))\n",
    "print(x_norm.var(dim=-1, unbiased=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5c1c17",
   "metadata": {},
   "source": [
    "### Step 5: Add scale (γ) and shift (β)\n",
    "\n",
    "Now we add trainable parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "944a88bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.2247,  0.0000,  1.2247]])\n"
     ]
    }
   ],
   "source": [
    "gamma = torch.tensor([1.0, 1.0, 1.0])  # scale\n",
    "beta  = torch.tensor([0.0, 0.0, 0.0])  # shift\n",
    "\n",
    "y = gamma * x_norm + beta\n",
    "print(y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98f7ce3",
   "metadata": {},
   "source": [
    "## Step 6: See why scale and shift matter\n",
    "\n",
    "Change them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.4495, -1.0000,  1.2247]])\n"
     ]
    }
   ],
   "source": [
    "gamma = torch.tensor([2.0, 0.5, 1.0])\n",
    "beta  = torch.tensor([1.0, -1.0, 0.0])\n",
    "\n",
    "y = gamma * x_norm + beta\n",
    "print(y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f996640e",
   "metadata": {},
   "source": [
    "Now each feature:\n",
    "\n",
    "1. Is scaled differently\n",
    "\n",
    "2. Is shifted differently\n",
    "\n",
    "The model learns these values during training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4051d8ee",
   "metadata": {},
   "source": [
    "## Step 7: Wrap it into a class (from scratch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SimpleLayerNorm(nn.Module):\n",
    "    def __init__(self, features):\n",
    "        super().__init__()\n",
    "        self.gamma = nn.Parameter(torch.ones(features))\n",
    "        self.beta  = nn.Parameter(torch.zeros(features))\n",
    "        self.eps = 1e-5\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var  = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        x_hat = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.gamma * x_hat + self.beta\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c186f95",
   "metadata": {},
   "source": [
    "##### Test it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "77ccc1a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.2247,  0.0000,  1.2247]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "ln = SimpleLayerNorm(3)\n",
    "print(ln(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b191219f",
   "metadata": {},
   "source": [
    "LayerNorm is always:\n",
    "\n",
    "for each sample:\n",
    "   1. mean over features\n",
    "   2. variance over features\n",
    "   3. normalise\n",
    "   4. scale\n",
    "   5. shift\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7283719",
   "metadata": {},
   "source": [
    "## Residual or - Shortcut or -  Skip - Connection\n",
    "\n",
    "\n",
    "A residual (skip) connection just means:\n",
    "\n",
    "“Take the input, do something to it, then add the original input back.”\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4089a47b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \n",
       "y = x + f(x)\n",
       "$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Math\n",
    "\n",
    "display(Math(r\"\"\"\n",
    "y = x + f(x)\n",
    "\"\"\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0693eff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "with out residual connection: tensor([2., 4., 6.])\n",
      "with residual connection: tensor([3., 6., 9.])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1.0, 2.0, 3.0])\n",
    "\n",
    "\n",
    "def f(x):\n",
    "    return 2 * x\n",
    "\n",
    "## With out residual connection\n",
    "\n",
    "y = f(x)\n",
    "print(\"with out residual connection:\", y)\n",
    "\n",
    "## with residual connection\n",
    "\n",
    "y = x + f(x)\n",
    "print(\"with residual connection:\", y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8abf5d",
   "metadata": {},
   "source": [
    "### 3. Why this matters (Intuition, not hype)\n",
    "If $f(x)$ learns something useless or noisy:\n",
    "*   **The model can simply ignore it**: The identity path ($x$) still survives.\n",
    "*   **Increased Robustness**: Residuals make deep networks significantly harder to \"break\" or degrade during training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ad7e40",
   "metadata": {},
   "source": [
    "-------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2fe41af",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "source": [
    "# 5. Residual + LayerNorm (Transformer-style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c8905920",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualLayerNorm(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(dim, dim)\n",
    "        self.ln = SimpleLayerNorm(dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.ln(x + self.linear(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54bfa439",
   "metadata": {},
   "source": [
    "# Why Transformers Need Residual Connections\n",
    "\n",
    "### ❌ Without Residuals\n",
    "*   **Vanishing Gradients:** Gradients shrink exponentially as they propagate backward, making updates negligible.\n",
    "*   **Early Layer Stagnation:** Initial layers stop learning because they receive little to no signal for optimization.\n",
    "*   **Training Instability:** Deep architectures become extremely difficult to converge, leading to performance degradation.\n",
    "\n",
    "### ✅ With Residuals\n",
    "*   **Unimpeded Information Flow:** Input features bypass transformations, ensuring the core signal is preserved across the network.\n",
    "*   **Gradient Highway:** Provides a direct \"shortcut\" for gradients to flow backward to earlier layers without distortion.\n",
    "*   **Deep Scalability:** Enables the training of massive models by maintaining signal strength regardless of depth.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c48cc2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
